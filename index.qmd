---
title: "Text As Data Blog Posts "
format: html
editor: visual
---

# Blog Post 1

## Introduction

My name is Sophie Ryan, I am a Data Analytics and Computational Social Sciences graduate student with a background in Communications and Media Studies and Psychology. My studies have inspired a passion for learning about how people interact with each other and the world around them. I love using data as a way to deepen my understanding of the social sciences from a new perspective.

In my undergraduate degree, I took a course called Countercultural Films. I was able to learn about the counterculture movement of the 1960's through the perspective of media and music. I believe music is one of the most powerful forms of communication we have, and it is truly reflective of social attitudes. During this time, rock music was extremely politically charged and played a huge role in giving young people a voice. After Kennedy's assassination, this political and cultural phenomenon spread anti-war, anti-establishment, and pro-peace messages through music and film. The pinnacle of this movement was the Woodstock Festival in 1969, where influential artists like Jimi Hendrix, Bob Dylan, and Joan Baez gave legendary performances that protested the government and promoted social justice. Hundreds of thousands of people came together at the festival to show solidarity. Moments like this display how important music can be in politics and culture. It acted not just as entertainment, but a catalyst for change and representation of public opinion.

## Research Question

My goal is to dive deeper into this political and social movement through the lense of text-as-data. How did the sentiment and themes of counterculture rock music differ from the mainstream political messages from the United States government, particularly in relation to the Vietnam War, social justice, and civil rights?

## Corpora

The corpora I plan to use for the music portion of this project will come from the Billboard Charts and Lyric Genius APIs. This will allow me to see the most popular and influential music, and access the lyrics to those songs to conduct further analysis. MSU's underground press and alternative newspaper collection could be used as a supplemental source representative of the counterculture movement's messages.

For my analysis of mainstream political messages from the time, I will use ProQuest's archives of historical newspapers. Additionally, I will investigate the Miller Center UVA speech archive, which provides transcripts of presidential speeches.

## Wrapping Up

I'm looking forward to using text analysis to dive deeper into how music was representative of the political and social unrest of the 1960's. Many of the lyrics from these artists openly criticized the government, the war, and racial injustice. The Lyric Genius corpora will be a great source for identifying themes, sentiments, and word frequencies. Mainstream newspaper articles will show how politics were being presented. Presidential speeches are also a good representation of the messages that the government was trying to convey at the time. I'm very intrigued to see the contrast of sentiments and themes between the two opposing groups. It could provide insight as to how the counterculture movement was so influential and worked to change public opinion on a variety of issues.

# Blog Post 2

# Counterculture: Music vs. Media

## Research Question

In my project, I will be investigating how themes and sentiments of counterculture rock music in the 1960's compared to that of mainstream media, particularly in relation to the Vietnam War, civil rights, and social justice movements.

## My Corpus

In order to collect this data, I decided to utilize the set list from the Woodstock Art and Music Festival of 1969. This festival was a pivotal moment in music history and popular culture. It was representative of the wide social unrest taking place at the time. Almost 500,000 people attended in efforts to promote peace and unity. This was the pinnacle of the counterculture movement, a political and cultural phenomenon that was anti-war, anti-establishment, and pro-peace. Young people across the country organized protests to fight for civil rights, and music often played a huge role in this. Many rock artists at this time used their platforms to support these causes, and the music was very politically charged. Jimi Hendrix' famous rendition of the Star Spangled Banner is one example of how the festival was leveraged to express unhappiness with the political climate. Therefore, the music played at this 3 day long festival is the perfect culmination of songs for my corpus representing the messages of counterculture's rock music.

### Lyrics Corpus

To acquire these songs and lyrics, I used webscraping from Wikipedia and a lyric website called LyricsFreak. First, I scraped the Wikipedia page that gave a list of songs and artists.

```{r}
#Loading packages
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(dplyr)
library(quanteda.textplots)
library(quanteda.sentiment)

#Defining the Wikipedia URL for the setlist at Woodstock
url1 <- "https://en.wikipedia.org/wiki/List_of_performances_and_events_at_Woodstock_Festival"

#Defining the CSS selectors. 
css_selector <- ".mw-body p + ol"
css_selector_artists <- ".mw-body .mw-heading3"
special_css <- "ol"
```

I defined 3 selectors, one for the titles which gave the artist name, another for the body which gave the song list, and a third for a special case, Crosby Stills and Nash. This artist's songs were in a nested list, which I had to treat differently.

```{r}
#Song text
song_list <- url1 %>%
  read_html() %>%
  html_nodes(css = css_selector) %>%
  html_text()
  
#CSN song text
special_case <- url1 %>%
  read_html() %>%
  html_nodes(css = special_css) %>%
  html_text() 

#Artist list
artist_list <- url1 %>%
  read_html() %>%
  html_nodes(css = css_selector_artists) %>%
  html_text()

#Removing a speaker, not an artist
artist_list <- artist_list[-2]

#Defining the nested list I need
special2 <- (special_case[29:32])
flattened_songs <- paste(special2, collapse = "\n")

#Adding the nested list songs to the original song list in the corresponding artist spot
song_list <- append(song_list, list(flattened_songs), after = 28)

# Remove [edit] from the artist names
artist_list <- gsub("\\[edit\\]", "", artist_list)
artist_list <- str_squish(artist_list)

split_songs <- str_split(song_list, "\n")

# Clean up whitespace and remove empty rows
clean_songs <- map(split_songs, ~ str_squish(.x) %>% .[. != ""])

# Only keep artists with corresponding song lists
# Check which artists actually have songs listed
valid_artists <- artist_list[1:length(clean_songs)]

# Combine only valid pairs
woodstock_data <- map2(
  valid_artists,
  clean_songs,
  ~ data.frame(artist = .x, song = .y, stringsAsFactors = FALSE)
) %>%
  bind_rows()

# Display the first 10 rows
head(woodstock_data, 10)
```

```{r}
library(dplyr)

# Replacing Crosby Stills Nash & Young artist name, because the lyric site names them as only "Crosby Stills & Nash" 
woodstock_data <- woodstock_data %>%
  mutate(artist = ifelse(artist == "Crosby, Stills, Nash & Young", 
                         "Crosby, Stills & Nash", 
                         artist))
```



Now I have a data frame consisting of every artist and each song they played. Next, I had to add lyrics to the data frame. I did this by creating a function that scraped the lyrics from pages on LyricsFreak for every song.

```{r}
#Loading packages
library(httr)
library(curl)

#Creating a function to scrape the LyricsFreak pages for each song. 
get_lyrics_lyricsfreak <- function(song_name, artist_name) {
  # Construct search URL
  base_search_url <- "https://www.lyricsfreak.com/search.php"
  query <- paste(artist_name, song_name)
  
  # Send search request
  search_page <- GET(base_search_url, query = list(q = query))
  
  # Parse the search result page
  search_html <- read_html(content(search_page, "text"))
  
  # Get the first song link that matches the artist
  song_link <- search_html %>%
    html_nodes("a.song") %>%
    html_attr("href") %>%
    .[1]  # Get the first match
  
  # If no link found, return NA
  if (is.na(song_link) || length(song_link) == 0) {
    return(NA)
  }
  
  # Complete the URL for the song's page
  song_url <- paste0("https://www.lyricsfreak.com", song_link)
  
  # Scrape the lyrics from the song's page
  song_page <- read_html(song_url)
  
  lyrics <- song_page %>%
    html_nodes(".js-share-text") %>%
    html_text2()
  
  # Clean up the lyrics text
  lyrics <- str_squish(lyrics)
  
  return(lyrics)
}


# Initialize a column to store lyrics
woodstock_data$lyrics <- NA

# Loop through each row and get lyrics
for (i in 1:nrow(woodstock_data)) {
  song_name <- woodstock_data$song[i]
  artist_name <- woodstock_data$artist[i]
  
  # Get the lyrics and store it in the new column
  woodstock_data$lyrics[i] <- get_lyrics_lyricsfreak(song_name, artist_name)
  
  # Pause to be polite to the website
  Sys.sleep(2)
}
```

```{r}
head(woodstock_data)
```

Running this function added an empty column to my existing data frame, and added the lyrics for each song. I am running into some problems at this step, as 161 of the songs do not have lyrics. Going forward, I may have to scrape from other websites to fill in gaps. For now, I have enough lyrics to create a lyrics corpus and perform some pre-processing steps.

```{r}
#Row 174 provided incorrect lyrics to The Who's song; "I'm Free", so I'm removing this row. 
woodstock_data <- woodstock_data[-174, ]
```


```{r}
#First I removed NA and missing values 
cleaned_data <- woodstock_data %>%
  filter(!is.na(lyrics) & lyrics != "")

cleaned_data$doc_id <- paste(cleaned_data$artist, cleaned_data$song, sep = " - ")

#Creating the corpus
lyrics_corpus <- corpus(cleaned_data$lyrics, 
                        docnames = cleaned_data$doc_id)

summary(lyrics_corpus)
ndoc(lyrics_corpus)
sum(ntoken(lyrics_corpus))
```

There are 156 documents and 33563 tokens in my corpus.

Next, I removed numbers, punctuation, and stopwords. I added some custom stopwords, as song lyrics tend to have some repetitive terms that are not very representative of the messages of the songs.

```{r}
tokens_clean <- tokens(lyrics_corpus, 
                       what = "word", 
                       remove_numbers = TRUE,
                       remove_punct = TRUE) %>%
                tokens_tolower()

custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "bah", "ahh", "shaka-laka-laka")

tokens_clean <- tokens_select(tokens_clean,
                              pattern = custom_stopwords,
                              selection = "remove")
```

Then, I created a document feature matrix and found some top features.

```{r}
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
```

Next, I created a few preliminary visualizations to get an idea of what I'll be working with. I made a wordcloud and a network. Moving forward, I'll have to figure out ways I can remove more custom "stopwords" that appear in almost all songs but do not add value.

```{r}
library(quanteda.textplots)

set.seed(1234)

# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
```

```{r}
smaller_dfm <- dfm_trim(lyrics_dfm, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .2, docfreq_type = "prop")

# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)

myFeatures <- names(topfeatures(smaller_dfm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_smaller_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

Finally I found some sentiment scores.

```{r}
sentiment_scores <- dfm_lookup(lyrics_dfm, dictionary = data_dictionary_LSD2015)

# convert to df
sentiment_df <- convert(sentiment_scores, to = "data.frame")

sentiment_df$Text <- as.character(lyrics_corpus)

head(sentiment_df)
```


There is still a lot of work that needs to be done on this corpus, but I believe I'm making good progress and this will be a good representation of the themes of counterculture music.

### Media Corpus

For my corpus of mainstream media about counterculture, I would like to use historical newspapers from ProQuest. I was exploring many of these documents using search terms such as "Vietnam War", "antiwar", "protest", "hippie", "counterculture", etc. There were hundreds of relevant documents. Unfortunately, most of these are in pdf form. This makes creating the corpus more difficult. Going forward, I will either have to perform OCR, or find another source to represent the sentiment of mainstream media during the 60's.

Here's an example of one newspaper article from April 24, 1971.

![](images/Screenshot 2025-02-26 at 5.01.13 PM.png)

Documents like this could be really useful in my analysis, and I look forward to continuing work on creating a corpus.

## Conclusion

Using text as data to investigate counterculture will provide a really interesting snapshot of sentiments and themes of the time period. I'm fascinated by this movement, and I believe it was a very important time in America for politics, culture, and rock music. Breaking down these corpora into their core elements can provide a new perspective on counterculture that I haven't previously considered, and I'm excited to see where this project can take me.


# Blog Post 3

## Research Question

In my project, I will be investigating how themes and sentiments of 1960's counterculture rock music contrasts to that of mainstream media, with a focus on key issues such as the Vietnam War, civil rights, and social justice movements.

## Exploratory Analysis

In order to uncover patterns in my data, I experimented with a variety of methods. Working with song lyrics has been challenging, but I've continued my work with sentiment analysis, and began to perform topic modeling. I'm also exploring key words in context (kwic) to get a better understanding. For my OCR work with ProQuest, I am still struggling with data wrangling. I've had to manually download PDFs, which is time consuming and I'm working on a better solution. Despite these challenges, I'm starting to see my data more clearly and uncover themes. 

### Lyric Analysis

First, I visualized overall sentiment scores of the song lyric corpus. These sentiments are pretty spread out, so I went into more depth looking at specific topics. 

```{r}
library(ggplot2)

sentiment_df$polarity <- (sentiment_df$positive - sentiment_df$negative)/(sentiment_df$positive + sentiment_df$negative)
ggplot(sentiment_df) +
  geom_histogram(aes(polarity)) +
  theme_bw()
```


#### Looking at some key words in context; "war", "peace", "free". 

I wanted to see how the lyrics relate to these topics that were pertinent to the counterculture movement. I created a couple of wordclouds with these key words. 

```{r}
kwic_war <- kwic(tokens_clean,
      pattern = c("war"))

# look at the first few uses
head(kwic_war)

# now look at a broader window of terms
kwic_war <- kwic(tokens_clean,
      pattern = c("war"),
      window = 10)

# look at the first few uses
head(kwic_war)

```

```{r}
kwic_peace <- kwic(tokens_clean,
      pattern = c("peace"))

# look at the first few uses
head(kwic_peace)

# now look at a broader window of terms 
kwic_peace <- kwic(tokens_clean,
      pattern = c("peace"),
      window = 10)

# look at the first few uses
head(kwic_peace)
```

```{r}
kwic_free <- kwic(tokens_clean,
      pattern = c("free"))

# look at the first few uses
head(kwic_free)

# now look at a broader window of terms 
kwic_free <- kwic(tokens_clean,
      pattern = c("free"),
      window = 10)

# look at the first few uses
head(kwic_free)
```

```{r}
freedom_corp <- corpus(kwic_free)

freedom_tokens <- tokens(freedom_corp)

free_dfm <- dfm(freedom_tokens)

textplot_wordcloud(free_dfm, min_count = 3, random_order = FALSE)
```


```{r}
peace_corp <- corpus(kwic_peace)

peace_tokens <- tokens(peace_corp)

peace_dfm <- dfm(peace_tokens)

textplot_wordcloud(peace_dfm, min_count = 3, random_order = FALSE)
```

```{r}
war_corp <- corpus(kwic_war)

war_tokens <- tokens(war_corp)

war_dfm <- dfm(war_tokens)

textplot_wordcloud(war_dfm, min_count = 3, random_order = FALSE)
```

I chose to look more into the "free" dfm, as it has the most data. I looked at sentiment scores for the "free" key word, and found the polarity to be overwhelmingly positive. The distribution here is much less evenly spread than in the overall corpus. 

```{r}
sentiment_scores_free <- dfm_lookup(free_dfm, dictionary = data_dictionary_LSD2015)

# convert to df
sentiment_df_free <- convert(sentiment_scores_free, to = "data.frame")

sentiment_df_free$Text <- as.character(freedom_corp)

head(sentiment_df_free)
```

```{r}
sentiment_df_free$polarity <- (sentiment_df_free$positive - sentiment_df_free$negative)/(sentiment_df_free$positive + sentiment_df_free$negative)
ggplot(sentiment_df_free) +
  geom_histogram(aes(polarity)) +
  theme_bw()
```

Overall, looking at key words has been helpful, especially when song lyrics tend to be more vague and abstract. Providing a focal point allows for a more focused approach. 

#### Topic modeling

I used topic modeling techniques to try to get a feel for topics in my corpus. This has proved challenging with song lyrics, and I needed to get creative with my stopwords. 

```{r}
# Installs text2vec package (might take a while)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages('text2vec')
```

```{r}
library(text2vec)
```


```{r}
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
head(tokens2, 2)
```


```{r}
# Iterates over each token
it <- itoken(tokens2, ids = cleaned_data$artist[1:350], progressbar = FALSE)

# Prints iterator
it
```

```{r}
# Built the vocabulary
v <- create_vocabulary(it)

# Print vocabulary
v
```

```{r}
v <- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)

# Check dimensions
dim(v)
```

```{r}
vectorizer <- vocab_vectorizer(v)
```

```{r}
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")
```


```{r}
lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)

# Print other methods for LDA
lda_model
```

```{r}
# Fitting model
doc_topic_distr <-
  lda_model$fit_transform(x = dtm, n_iter = 1000,
                          convergence_tol = 0.001, n_check_convergence = 25,
                          progressbar = FALSE)
```

```{r}
barplot(doc_topic_distr[1, ], xlab = "topic",
        ylab = "proportion", ylim = c(0, 1),
        names.arg = 1:ncol(doc_topic_distr))
```


```{r}
lda_model$get_top_words(n = 10, topic_number = c(3L, 5L, 6L), lambda = 0.4)
```


```{r}
install.packages("stm")
library(stm)
```

```{r}
custom_stopwords <- c(stopwords("en"), "baby", "sha", "yum", "yip", "nana", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey","bah", "ahh", "shaka-laka-laka")
all_stopwords <- c(stopwords("english"), custom_stopwords)

# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
  remove_numbers = T,
  remove_punct =  T,
  remove_symbols = T)

# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
  all_stopwords)

# create dfm
myDfm <- dfm(lyric_tokens,
  tolower=T)

dim(myDfm)
```

```{r}
cor_topic_model <- stm(myDfm, K = 5,
                   verbose = FALSE, init.type = "Spectral")
```

```{r}
labelTopics(cor_topic_model)
```

```{r}
findThoughts(cor_topic_model,
    texts = sentiment_df$Text,
    topics = c(1:5),
    n = 1)
```

```{r, include=FALSE, message=FALSE, warning=FALSE}
# choose our number of topics
k <- 5

# specify model
myModel <- invisible(
  suppressMessages(
    suppressWarnings(
      stm(myDfm,
            K = k,
            prevalence =~ positive * negative,
            data = sentiment_df,
            max.em.its = 1000,
            seed = 1234,
            init.type = "Spectral")
      )
    )
  )

```

```{r}
labelTopics(myModel)
```

```{r}
plot(myModel, type = "summary")
```

```{r}
# get the words
myTopicNames <- labelTopics(myModel, n=4)$frex

# set up an empty vector
myTopicLabels <- rep(NA, k)

# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
	myTopicLabels[i] <- paste(myTopicNames[i,], collapse = "_")
}

# print the names
myTopicLabels
```

There seems to be prevalence with themes like "love", "share", "home", "time". I've found that I need to continually tweak my code to account for words in songs that aren't really words, and this has made it more difficult to uncover overall themes that are often represented in abstract ways. Because of this, it's helpful to incorporate analysis on historical newspapers from the time of the counterculture movement to see how mainstream media is looking at topics such as the Vietnam war.  

### Media Analysis

I'm looking at historical newspapers from before 1975 pertaining to topics such as "war", "protest", "anti-war", "Vietnam". ProQuest provided me with hundreds of documents, some of which I've entered into R and performed OCR to create a preliminary corpus. I had to add some custom stopwords here as well, because OCR can't always pick up on all of the words in blurry, older documents. 

```{r}
install.packages(c("tesseract", "pdftools"))
library(tesseract)
library(pdftools)
```

```{r}
pdf_file <- c("/Users/sophieryan/Documents/Github/Blogs/Extremist_Activities_Are_Threa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Nixon_Says_It's_Time_to_Draw_L.pdf", "/Users/sophieryan/Documents/Github/Blogs/Most_of_U.S._is_behind_anti-wa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Bravery_medals_discarded_in_an.pdf", "/Users/sophieryan/Documents/Github/Blogs/War_Protesters_Mass_For_Washin-2.pdf",
"/Users/sophieryan/Documents/Github/Blogs/250,000_WAR_PROTESTERS_STAGE_P.pdf", "/Users/sophieryan/Documents/Github/Blogs/WAR_RALLY_TODAY_EXPECTS_NEW_AI.pdf","/Users/sophieryan/Documents/Github/Blogs/Agnew_Scores_War_Foes;_Rally_t.pdf", "/Users/sophieryan/Documents/Github/Blogs/EXECUTIVE_URGES_UNITY_WITH_YOU.pdf", "/Users/sophieryan/Documents/Github/Blogs/Watching_the_Returns_Some_Rac.pdf")  

```

```{r}
#Checking that the pdfs work in my environment
file.exists(pdf_file)
```

```{r}
text <- ocr(pdf_file)
```

```{r}
writeLines(text, "ocr_output.txt")
```

```{r}
newspaper_corpus <- corpus(text)
```

```{r}
summary(newspaper_corpus)
ndoc(newspaper_corpus)
sum(ntoken(newspaper_corpus))
```

```{r}
news_tokens_clean <- tokens(newspaper_corpus, 
                       what = "word", 
                       remove_numbers = TRUE,
                       remove_punct = TRUE) %>%
                tokens_tolower()

news_custom_stopwords <- c(stopwords("en"), "ee", "ae", "said", "se", "re", "oe", "eee", "e", "es", "I")

news_tokens_clean <- tokens_select(news_tokens_clean,
                              pattern = news_custom_stopwords,
                              selection = "remove")

```

```{r}
news_dfm <- dfm(news_tokens_clean)
topfeatures(news_dfm)
```
This wordcloud helps visualize the dfm overall. 

```{r}
library(quanteda.textplots)

set.seed(1234)

# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 10, random_order = FALSE)
```

```{r}
sentiment_scores_news <- dfm_lookup(news_dfm, dictionary = data_dictionary_LSD2015)

# convert to df
sentiment_df_news <- convert(sentiment_scores_news, to = "data.frame")

sentiment_df_news$Text <- as.character(newspaper_corpus)

head(sentiment_df_news)
```
```{r}
library(ggplot2)
sentiment_df_news$polarity <- (sentiment_df_news$positive - sentiment_df_news$negative)/(sentiment_df_news$positive + sentiment_df_news$negative)
ggplot(sentiment_df_news) +
  geom_histogram(aes(polarity)) +
  theme_bw()
```
The sentiments in the newspaper corpus are much more negative, and are not as evenly spread as the song lyric corpus. 

```{r}
news_fcm <- fcm(news_dfm)
myFeatures <- names(topfeatures(news_dfm, 30))

# retain only those top features as part of our matrix
smaller_fcm_news <- fcm_select(news_fcm, pattern = myFeatures, selection = "keep")


# compute size weight for vertices in network
size <- log(colSums(smaller_fcm_news))

# create plot
textplot_network(smaller_fcm_news, vertex_size = size / max(size) * 3)
```
I looked at the keyword "peace" 

```{r}
kwic_news_peace <- kwic(news_tokens_clean,
      pattern = c("peace"))

# look at the first few uses
head(kwic_news_peace)

# now look at a broader window of terms 
kwic_news_peace <- kwic(news_tokens_clean,
      pattern = c("peace"),
      window = 10)

# look at the first few uses
head(kwic_news_peace)
```


```{r}
news_peace_corp <- corpus(kwic_news_peace)

news_peace_tokens <- tokens(news_peace_corp)

news_peace_dfm <- dfm(news_peace_tokens)

textplot_wordcloud(news_peace_dfm, min_count = 3, random_order = FALSE)
```

I'm starting to look at key words in context here as well. As I move forward, I want to directly compare themes in each corpus as they relate to particular words. I think seeing differences in sentiments in this way will provide more insight as to how the themes compared. It is clear that a lot of the tokens related to "students", "peace" and "protesters". I'm excited to see what further analysis may reveal about these two corpora. 

## Reflection

Overall, this exploration of my data has provided me with a much better understanding of how I should proceed with this interesting but complex set of corpora. For my lyric corpus, I may need to find a way to add more data, and work on removing more random stop words. I think the topic modeling was helpful, but it did make me realize that song lyrics need to be treated much differently than other more traditional documents. This being said, incorporating the media data may help in making sense of the song themes. I think it will be important moving forward to focus on common key words in both datasets and use that as my way of comparison. I need to continue developing my media corpus by adding more documents. I will also do topic modeling on that corpus. In my final product, I'd like to include word clouds and/or text plot networks that represent both corpora in one single visualization. I think this will provide a much more well rounded understanding of how they relate. This project has been very enjoyable for me so far. I think investigations like this are important. In order to fully understand the attitudes, beliefs, and behaviors of a time period, we can't just look at official documents and mainstream published media. We need to understand how everybody was feeling, and music is often a voice to those who don't have the power and connections to make themselves heard. Particularly in the counterculture movement, art could be just as political as newspapers. I look forward to working more with this data to provide a well rounded, different perspective of the time period. 

# Blog Post 4

## Summary 

As my research project progresses, I've experimented with many different methods to help make sense of my two corpora. My research question aims to investigate how themes and attitudes of 1960's counterculture music compares or contrasts to the sentiment of mainstream media of the time. My data set comes from two sources. Firstly, I had to acquire text that is representative of the music from 1960's counterculture. I decided that the set list from Woodstock would be the perfect way to capture this, considering it was the pinnacle of the movement. The festival was created as a way to promote peace and freedom, and give a voice to young people protesting the Vietnam war and fighting for civil rights. I scraped the list of artists and songs from Wikipedia, then scraped a lyric website to append lyrics to a data frame. I created my corpus out of that data. My second source is ProQuest, where I've looked at historical newspapers from before 1975 that relate to key words, such as "Vietnam war", "protest", "hippies". I used OCR on these documents to create the newspaper corpus. This was somewhat challenging, as I had to manually input the pdf links. As this has been time consuming, I will be adding more links before completing the final product. To address my research question, I've tried a variety of techniques. These include sentiment analysis, clustering, LDA and STM topic modeling. I've created a handful of textplot networks, wordclouds, and histograms to help visualize my results. These methods have helped give me insight on the main themes of my data, and built a solid foundation as I move to the final. 

## Findings 

In my work so far, I've been looking a lot at key words in context. With song lyrics, it can be difficult to understand the results of text analysis. Many of the most common words are not meaningful. For example, I've had to create a lot of custom stopwords, otherwise my findings would be almost entirely "hey", "yeah", "oh", etc. This doesn't mean that the lyrics have not given me interesting results. In my STM topic modeling, The top 5 topics produced some text that I found to be very representative of the main ideas that the counterculture movement was trying to express: 

"Take a sister by her hand Lead her far from this barren land Horror grips us as we watch you die All we can do is echo your anguished cry and Stare as all you human feelings die We are leaving You don't need us Go and take a sister by her hand Lead her far from this foreign land Somewhere where we might laugh again We are leaving You don't need us Sailing ships on the water very free and easy Easy you know the way it's supposed to be Silver people on the shoreline leave us be Very free And gone"

"Hey, it's a long hard road, it's a long hard road It's a long hard road, hey, before we'll be free Hey, before we'll be free Hey, look yonder, tell me what you see Marching to the fields of Vietnam? Looks like Handsome Johnny with an M15 Marching to the Vietnam war, hey, marching to the Vietnam war [...] Hey, what's the use of singing this song Some of you are not even listening Tell me what it is we've got to do"

"We shall overcome, We shall overcome, some day. Oh, deep in my heart, I do believe We shall overcome, some day. We'll walk hand in hand, We'll walk hand in hand, We'll walk hand in hand, some day. Oh, deep in my heart, We shall live in peace, We shall live in peace, We shall live in peace, some day. Oh, deep in my heart, We shall all be free, We shall all be free, We shall all be free, some day."

The first topic shows themes of war, expressing anguish, sadness, and a hope for freedom. The next topic is explicitly talking about Vietnam, feeling rather hopeless, and wondering what's the point of the song if the people don't feel like they are being heard or taken care of. The last topic is a song of faith, that one day the people will be free and live in peace. I thought it was really amazing that the topic modeling was able to produce such succinct summaries of what I was hoping to represent with this analysis. Of course, it isn't perfect, and in fact the last topic in my model was made up of 90% the word "ya". I'll need to continue to fine tune my stopwords for the final project. 

At the end of my last post, I discussed wanting to visualize wordclouds that represent both corpora in one plot, as an easier way to really compare and contrast the themes of each. I've included two of those here. One contains the entire lyric and newspaper dfms, and the other contains smaller dfms from the key word "peace". 

```{r}
lyric_dfm_grouped <- dfm_group(lyrics_dfm, groups = rep("lyrics", ndoc(lyrics_dfm)))
news_dfm_grouped   <- dfm_group(news_dfm, groups = rep("news", ndoc(news_dfm)))


# Combine them into one
dfm_combined_ <- rbind(lyric_dfm_grouped, news_dfm_grouped)

# Create a comparison word cloud
textplot_wordcloud(dfm_combined_,
                   comparison = TRUE,
                   max_words = 100,
                   color = c("blue", "red"))
```

```{r}
library(quanteda)
library(quanteda.textplots)

# Add grouping variable to each dfm
peace_dfm_grouped <- dfm_group(peace_dfm, groups = rep("peace", ndoc(peace_dfm)))
news_peace_dfm_grouped   <- dfm_group(news_peace_dfm, groups = rep("news_peace", ndoc(news_peace_dfm)))


# Combine them into one
dfm_combined <- rbind(peace_dfm_grouped, news_peace_dfm_grouped)

# Create a comparison word cloud
textplot_wordcloud(dfm_combined,
                   comparison = TRUE,
                   max_words = 200,
                   color = c("blue", "red"))
```

I think these wordclouds are interesting, and a good choice of visualization with respect to my research question. 

## Moving Forward

I have a couple of areas I need to work on as I head into the final stretch. I am still concerned about my lyric corpus, and how it can sometimes be lacking "meat". Readjusting for more variations of the word "yeah" can be time consuming and slightly frustrating. For my newspaper corpus, I still need to add a lot more documents to give me a more well rounded set of texts. I will then proceed with topic modeling on that corpus to see what insights it may reveal. I'd like to keep focusing on comparing key words in context. I believe that will help me stay focused on the goal of the research, particularly with my somewhat messy lyrics. I'm a little worried that the corpus won't be able to provide me with enough real results to draw any sort of reliable conclusions. My plan moving forward is to fine tune the code I've already written, as well as expand on my newspaper corpus. I'm happy with my progress so far, and I think the results are intriguing. This is a topic that I'm really passionate about, so understanding it through the lens of text-as-data has been challenging, but fun. I'm hoping the final results can give a good, well-rounded representation of different points of view of the counterculture movement. Understanding the perspective of both mainstream media and artistic representation is really important in order to grasp the movement as a whole. 


