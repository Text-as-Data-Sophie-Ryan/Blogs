custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x")
tokens_clean <- tokens_select(tokens_clean,
pattern = custom_stopwords,
selection = "remove")
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
smaller_dfm <- dfm_trim(lyrics_dfm, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .2, docfreq_type = "prop")
# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)
# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)
myFeatures <- names(topfeatures(smaller_dfm, 30))
# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")
# check dimensions
dim(even_smaller_fcm)
# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))
# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
sentiment_scores <- dfm_lookup(lyrics_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df <- convert(sentiment_scores, to = "data.frame")
sentiment_df$Text <- as.character(lyrics_corpus)
head(sentiment_df)
woodstock_data <- woodstock_data[-174, ]
#First I removed NA and missing values
cleaned_data <- woodstock_data %>%
filter(!is.na(lyrics) & lyrics != "")
cleaned_data$doc_id <- paste(cleaned_data$artist, cleaned_data$song, sep = " - ")
#Creating the corpus
lyrics_corpus <- corpus(cleaned_data$lyrics,
docnames = cleaned_data$doc_id)
summary(lyrics_corpus)
ndoc(lyrics_corpus)
sum(ntoken(lyrics_corpus))
tokens_clean <- tokens(lyrics_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x")
tokens_clean <- tokens_select(tokens_clean,
pattern = custom_stopwords,
selection = "remove")
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
smaller_dfm <- dfm_trim(lyrics_dfm, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .2, docfreq_type = "prop")
# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)
# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)
myFeatures <- names(topfeatures(smaller_dfm, 30))
# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")
# check dimensions
dim(even_smaller_fcm)
# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))
# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
sentiment_scores <- dfm_lookup(lyrics_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df <- convert(sentiment_scores, to = "data.frame")
sentiment_df$Text <- as.character(lyrics_corpus)
head(sentiment_df)
# Installs text2vec package (might take a while)
install.packages('text2vec')
library(text2vec)
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
head(tokens2, 2)
head(cleaned_data)
# Iterates over each token
install.packages("text2vec")
it <- itoken(tokens2, ids = cleaned_data$artist[1:350], progressbar = FALSE)
# Prints iterator
it
# Built the vocabulary
v <- create_vocabulary(it)
# Print vocabulary
v
v <- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)
# Check dimensions
dim(v)
vectorizer <- vocab_vectorizer(v)
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
# Print other methods for LDA
lda_model
# Fitting model
doc_topic_distr <-
lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
barplot(doc_topic_distr[1, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 1)
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 0.4)
install.packages("stm")
library(stm)
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
install.packages("stm")
remove_punct =  T,
remove_symbols = T)
lda_model$get_top_words(n = 10, topic_number = c(4L, 8L, 10L), lambda = 1)
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 0.4)
install.packages("stm")
library(stm)
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
findThoughts(cor_topic_model,
texts = sentiment_df$Text,
topics = c(1:5),
n = 1)
custom_stopwords <- c(stopwords("en"), "baby", "sha", "yum", "yip" "nana", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
custom_stopwords <- c(stopwords("en"), "baby", "sha", "yum", "yip", "nana", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
findThoughts(cor_topic_model,
texts = sentiment_df$Text,
topics = c(1:5),
n = 1)
# choose our number of topics
k <- 5
# specify model
myModel <- stm(myDfm,
K = k,
prevalence =~ positive * negative,
data = sentiment_df,
max.em.its = 1000,
seed = 1234,
init.type = "Spectral")
labelTopics(myModel)
plot(myModel, type = "summary")
# get the words
myTopicNames <- labelTopics(myModel, n=4)$frex
# set up an empty vector
myTopicLabels <- rep(NA, k)
# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
myTopicLabels[i] <- paste(myTopicNames[i,], collapse = "_")
}
# print the names
myTopicLabels
# estimate effects
modelEffects <- estimateEffect(formula=1:k~positive,
stmobj = myModel,
metadata = sentiment_df)
# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
plot.estimateEffect(modelEffects,
covariate ="positive",
xlim=c(-.25,.25),
model = myModel,
topics = modelEffects$topics[i],
method = "difference",
cov.value1 = 1,
cov.value2=0,
main = myTopicLabels[i],
printlegend=F,
linecol="grey26",
labeltype="custom",
verbose.labels=F,
custom.labels=c(""))
par(new=F)
}
library(ggplot2)
sentiment_df$polarity <- (sentiment_df$positive - sentiment_df$negative)/(sentiment_df$positive + sentiment_df$negative)
ggplot(sentiment_df) +
geom_histogram(aes(polarity)) +
theme_bw()
kwic_war <- kwic(tokens_clean,
pattern = c("war"))
# look at the first few uses
head(kwic_war)
# now look at a broader window of terms
kwic_war <- kwic(tokens_clean,
pattern = c("war"),
window = 10)
# look at the first few uses
head(kwic_war)
kwic_peace <- kwic(tokens_clean,
pattern = c("peace"))
# look at the first few uses
head(kwic_peace)
# now look at a broader window of terms
kwic_peace <- kwic(tokens_clean,
pattern = c("peace"),
window = 10)
# look at the first few uses
head(kwic_peace)
kwic_free <- kwic(tokens_clean,
pattern = c("free"))
# look at the first few uses
head(kwic_free)
# now look at a broader window of terms
kwic_free <- kwic(tokens_clean,
pattern = c("free"),
window = 10)
# look at the first few uses
head(kwic_free)
freedom_corp <- corpus(kwic_free)
freedom_tokens <- tokens(freedom_corp)
free_dfm <- dfm(freedom_tokens)
textplot_wordcloud(free_dfm, min_count = 3, random_order = FALSE)
peace_corp <- corpus(kwic_peace)
peace_tokens <- tokens(peace_corp)
peace_dfm <- dfm(peace_tokens)
textplot_wordcloud(peace_dfm, min_count = 3, random_order = FALSE)
war_corp <- corpus(kwic_war)
war_tokens <- tokens(war_corp)
war_dfm <- dfm(war_tokens)
textplot_wordcloud(war_dfm, min_count = 5, random_order = FALSE)
sentiment_scores_free <- dfm_lookup(free_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df_free <- convert(sentiment_scores_free, to = "data.frame")
sentiment_df_free$Text <- as.character(freedom_corp)
head(sentiment_df_free)
sentiment_df_free$polarity <- (sentiment_df_free$positive - sentiment_df_free$negative)/(sentiment_df_free$positive + sentiment_df_free$negative)
ggplot(sentiment_df_free) +
geom_histogram(aes(polarity)) +
theme_bw()
war_corp <- corpus(kwic_war)
war_tokens <- tokens(war_corp)
war_dfm <- dfm(war_tokens)
textplot_wordcloud(war_dfm, min_count = 3, random_order = FALSE)
sentiment_scores_free <- dfm_lookup(free_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df_free <- convert(sentiment_scores_free, to = "data.frame")
sentiment_df_free$Text <- as.character(freedom_corp)
head(sentiment_df_free)
sentiment_df_free$polarity <- (sentiment_df_free$positive - sentiment_df_free$negative)/(sentiment_df_free$positive + sentiment_df_free$negative)
ggplot(sentiment_df_free) +
geom_histogram(aes(polarity)) +
theme_bw()
lda_model$get_top_words(n = 10, topic_number = c(4L, 8L, 10L), lambda = 0.4)
custom_stopwords <- c(stopwords("en"), "baby", "sha", "yum", "yip", "nana", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
# choose our number of topics
k <- 5
# specify model
myModel <- stm(myDfm,
K = k,
prevalence =~ positive * negative,
data = sentiment_df,
max.em.its = 300,
seed = 1234,
init.type = "Spectral")
labelTopics(myModel)
custom_stopwords <- c(stopwords("en"), "baby", "sha", "yum", "yip", "nana", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey","bah", "ahh", "shaka-laka-laka")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
findThoughts(cor_topic_model,
texts = sentiment_df$Text,
topics = c(1:5),
n = 1)
# choose our number of topics
k <- 5
# specify model
myModel <- stm(myDfm,
K = k,
prevalence =~ positive * negative,
data = sentiment_df,
max.em.its = 1000,
seed = 1234,
init.type = "Spectral")
labelTopics(myModel)
plot(myModel, type = "summary")
# get the words
myTopicNames <- labelTopics(myModel, n=4)$frex
# set up an empty vector
myTopicLabels <- rep(NA, k)
# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
myTopicLabels[i] <- paste(myTopicNames[i,], collapse = "_")
}
# print the names
myTopicLabels
tokens_clean <- tokens(lyrics_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "bah", "ahh", "shaka-laka-laka")
tokens_clean <- tokens_select(tokens_clean,
pattern = custom_stopwords,
selection = "remove")
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
head(tokens2, 2)
# Iterates over each token
it <- itoken(tokens2, ids = cleaned_data$artist[1:350], progressbar = FALSE)
# Prints iterator
it
# Built the vocabulary
v <- create_vocabulary(it)
# Print vocabulary
v
v <- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)
# Check dimensions
dim(v)
vectorizer <- vocab_vectorizer(v)
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
# Print other methods for LDA
lda_model
# Fitting model
doc_topic_distr <-
lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
barplot(doc_topic_distr[1, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
lda_model$get_top_words(n = 10, topic_number = c(3L, 5L, 6L), lambda = 0.4)
pdf_file <- c("/Users/sophieryan/Documents/Github/Blogs/Extremist_Activities_Are_Threa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Nixon_Says_It's_Time_to_Draw_L.pdf", "/Users/sophieryan/Documents/Github/Blogs/Most_of_U.S._is_behind_anti-wa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Bravery_medals_discarded_in_an.pdf", "/Users/sophieryan/Documents/Github/Blogs/War_Protesters_Mass_For_Washin-2.pdf",
"/Users/sophieryan/Documents/Github/Blogs/250,000_WAR_PROTESTERS_STAGE_P.pdf", "/Users/sophieryan/Documents/Github/Blogs/WAR_RALLY_TODAY_EXPECTS_NEW_AI.pdf","/Users/sophieryan/Documents/Github/Blogs/Agnew_Scores_War_Foes;_Rally_t.pdf", "/Users/sophieryan/Documents/Github/Blogs/EXECUTIVE_URGES_UNITY_WITH_YOU.pdf", "/Users/sophieryan/Documents/Github/Blogs/Watching_the_Returns_Some_Rac.pdf")
#Checking that the pdfs work in my environment
file.exists(pdf_file)
text <- ocr(pdf_file)
install.packages(c("tesseract", "pdftools"))
library(tesseract)
library(pdftools)
text <- ocr(pdf_file)
# Display the extracted text
cat(text)
writeLines(text, "ocr_output.txt")
newspaper_corpus <- corpus(text)
summary(newspaper_corpus)
ndoc(newspaper_corpus)
sum(ntoken(newspaper_corpus))
news_tokens_clean <- tokens(newspaper_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
news_custom_stopwords <- c(stopwords("en"), "ee", "ae", "said", "se", "re", "oe", "eee", "e", "es", "I")
news_tokens_clean <- tokens_select(news_tokens_clean,
pattern = news_custom_stopwords,
selection = "remove")
news_dfm <- dfm(news_tokens_clean)
topfeatures(news_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 10, random_order = FALSE)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 5, random_order = FALSE)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 10, random_order = FALSE)
sentiment_scores_news <- dfm_lookup(news_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df_news <- convert(sentiment_scores_news, to = "data.frame")
sentiment_df_news$Text <- as.character(newspaper_corpus)
head(sentiment_df_news)
library(ggplot2)
sentiment_df_news$polarity <- (sentiment_df_news$positive - sentiment_df_news$negative)/(sentiment_df_news$positive + sentiment_df_news$negative)
ggplot(sentiment_df_news) +
geom_histogram(aes(polarity)) +
theme_bw()
news_fcm <- fcm(news_dfm)
myFeatures <- names(topfeatures(news_dfm, 30))
# retain only those top features as part of our matrix
smaller_fcm_news <- fcm_select(news_fcm, pattern = myFeatures, selection = "keep")
# retain only those top features as part of our matrix
smaller_fcm_news <- fcm_select(news_fcm, pattern = myFeatures, selection = "keep")
# compute size weight for vertices in network
size <- log(colSums(smaller_fcm_news))
# compute size weight for vertices in network
size <- log(colSums(smaller_fcm_news))
# create plot
textplot_network(smaller_fcm_news, vertex_size = size / max(size) * 3)
news_fcm <- fcm(news_dfm)
myFeatures <- names(topfeatures(news_dfm, 30))
# retain only those top features as part of our matrix
smaller_fcm_news <- fcm_select(news_fcm, pattern = myFeatures, selection = "keep")
# compute size weight for vertices in network
size <- log(colSums(smaller_fcm_news))
# create plot
textplot_network(smaller_fcm_news, vertex_size = size / max(size) * 3)
kwic_news_peace <- kwic(news_tokens_clean,
pattern = c("peace"))
# look at the first few uses
head(kwic_news_peace)
# now look at a broader window of terms
kwic_news_peace <- kwic(news_tokens_clean,
pattern = c("peace"),
window = 10)
# look at the first few uses
head(kwic_news_peace)
news_peace_corp <- corpus(kwic_news_peace)
news_peace_tokens <- tokens(news_peace_corp)
news_peace_dfm <- dfm(news_peace_tokens)
textplot_wordcloud(news_peace_dfm, min_count = 3, random_order = FALSE)
kwic_news_hippie <- kwic(news_tokens_clean,
pattern = c("hippie"))
# look at the first few uses
head(kwic_news_hippie)
# now look at a broader window of terms
kwic_news_hippie <- kwic(news_tokens_clean,
pattern = c("hippie"),
window = 10)
# look at the first few uses
head(kwic_news_hippie)
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
# Installs text2vec package (might take a while)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages('text2vec')
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
# Installs text2vec package (might take a while)
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages('text2vec')
install.packages("text2vec")
View(kwic_peace)
View(modelEffects)
View(modelEffects)
View(myDfm)
