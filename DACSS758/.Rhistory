artist_list <- gsub("\\[edit\\]", "", artist_list)
artist_list <- str_squish(artist_list)
split_songs <- str_split(song_list, "\n")
# Clean up whitespace and remove empty rows
clean_songs <- map(split_songs, ~ str_squish(.x) %>% .[. != ""])
# Only keep artists with corresponding song lists
# Check which artists actually have songs listed
valid_artists <- artist_list[1:length(clean_songs)]
# Combine only valid pairs
woodstock_data <- map2(
valid_artists,
clean_songs,
~ data.frame(artist = .x, song = .y, stringsAsFactors = FALSE)
) %>%
bind_rows()
# Display the first 10 rows
head(woodstock_data, 10)
#Loading packages
library(httr)
library(curl)
#Creating a function to scrape the LyricsFreak pages for each song.
get_lyrics_lyricsfreak <- function(song_name, artist_name) {
# Construct search URL
base_search_url <- "https://www.lyricsfreak.com/search.php"
query <- paste(artist_name, song_name)
# Send search request
search_page <- GET(base_search_url, query = list(q = query))
# Parse the search result page
search_html <- read_html(content(search_page, "text"))
# Get the first song link that matches the artist
song_link <- search_html %>%
html_nodes("a.song") %>%
html_attr("href") %>%
.[1]  # Get the first match
# If no link found, return NA
if (is.na(song_link) || length(song_link) == 0) {
return(NA)
}
# Complete the URL for the song's page
song_url <- paste0("https://www.lyricsfreak.com", song_link)
# Scrape the lyrics from the song's page
song_page <- read_html(song_url)
lyrics <- song_page %>%
html_nodes(".js-share-text") %>%
html_text2()
# Clean up the lyrics text
lyrics <- str_squish(lyrics)
return(lyrics)
}
# Initialize a column to store lyrics
woodstock_data$lyrics <- NA
# Loop through each row and get lyrics
for (i in 1:nrow(woodstock_data)) {
song_name <- woodstock_data$song[i]
artist_name <- woodstock_data$artist[i]
# Get the lyrics and store it in the new column
woodstock_data$lyrics[i] <- get_lyrics_lyricsfreak(song_name, artist_name)
# Pause to be polite to the website
Sys.sleep(2)
}
head(woodstock_data)
#First I removed NA and missing values
cleaned_data <- woodstock_data %>%
filter(!is.na(lyrics) & lyrics != "")
cleaned_data$doc_id <- paste(cleaned_data$artist, cleaned_data$song, sep = " - ")
#Creating the corpus
lyrics_corpus <- corpus(cleaned_data$lyrics,
docnames = cleaned_data$doc_id)
summary(lyrics_corpus)
ndoc(lyrics_corpus)
sum(ntoken(lyrics_corpus))
tokens_clean <- tokens(lyrics_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x")
tokens_clean <- tokens_select(tokens_clean,
pattern = custom_stopwords,
selection = "remove")
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
smaller_dfm <- dfm_trim(lyrics_dfm, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .2, docfreq_type = "prop")
# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)
# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)
myFeatures <- names(topfeatures(smaller_dfm, 30))
# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")
# check dimensions
dim(even_smaller_fcm)
# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))
# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
sentiment_scores <- dfm_lookup(lyrics_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df <- convert(sentiment_scores, to = "data.frame")
sentiment_df$Text <- as.character(lyrics_corpus)
head(sentiment_df)
tokens_clean
cleaned_data$lyrics
class(cleaned_data)
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x")
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- tokens_select(tokens2,
pattern = custom_stopwords,
selection = "remove")
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- tokens_remove(custom_stopwords)
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
# Installs text2vec package (might take a while)
install.packages('text2vec')
install.packages("text2vec")
library(text2vec)
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
head(tokens2, 2)
head(lyrics_corpus)
head(lyrics_corpus)
View(lyrics_dfm)
class(tokens_clean)
class(tokens2)
tokens2 <- tolower(cleaned_data$lyrics)
tokens2 <- word_tokenizer(tokens2)
head(tokens2, 2)
# Iterates over each token
it <- itoken(tokens2, ids = cleaned_data$id[1:350], progressbar = FALSE)
# Prints iterator
it
head(cleaned_data)
# Iterates over each token
it <- itoken(tokens2, ids = cleaned_data$artist[1:350], progressbar = FALSE)
# Prints iterator
it
# Built the vocabulary
v <- create_vocabulary(it)
# Print vocabulary
v
v <- prune_vocabulary(v, term_count_min = 10, doc_proportion_max = 0.2)
# Check dimensions
dim(v)
vectorizer <- vocab_vectorizer(v)
dtm <- create_dtm(it, vectorizer, type = "dgTMatrix")
lda_model <- LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
# Print other methods for LDA
lda_model
# Fitting model
doc_topic_distr <-
lda_model$fit_transform(x = dtm, n_iter = 1000,
convergence_tol = 0.001, n_check_convergence = 25,
progressbar = FALSE)
barplot(doc_topic_distr[1, ], xlab = "topic",
ylab = "proportion", ylim = c(0, 1),
names.arg = 1:ncol(doc_topic_distr))
lda_model$get_top_words(n = 10, topic_number = c(1L, 5L, 10L), lambda = 1)
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 0.4)
install.packages("stm")
library(stm)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
stopwords("english"))
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
cor_topic_model <- stm(myDfm, K = 5,
verbose = FALSE, init.type = "Spectral")
labelTopics(cor_topic_model)
findThoughts(cor_topic_model,
texts = movie_review$review,
topics = c(1:5),
n = 1)
findThoughts(cor_topic_model,
texts = movie_review$review,
topics = c(1:4),
n = 1)
findThoughts(cor_topic_model,
texts = sentiment_df$Text,
topics = c(1:5),
n = 1)
# choose our number of topics
k <- 5
# specify model
myModel <- stm(myDfm,
K = k,
prevalence =~ positive * negative,
data = sentiment_df,
max.em.its = 1000,
seed = 1234,
init.type = "Spectral")
labelTopics(myModel)
plot(myModel, type = "summary")
# get the words
myTopicNames <- labelTopics(myModel, n=4)$frex
# set up an empty vector
myTopicLabels <- rep(NA, k)
# set up a loop to go through the topics and collapse the words to a single name
for (i in 1:k){
myTopicLabels[i] <- paste(myTopicNames[i,], collapse = "_")
}
# print the names
myTopicLabels
# estimate effects
modelEffects <- estimateEffect(formula=1:k~positive*negative,
stmobj = myModel,
metadata = sentiment_df)
# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
plot.estimateEffect(modelEffects,
covariate ="sentiment",
xlim=c(-.25,.25),
model = myModel,
topics = modelEffects$topics[i],
method = "difference",
cov.value1 = 1,
cov.value2=0,
main = myTopicLabels[i],
printlegend=F,
linecol="grey26",
labeltype="custom",
verbose.labels=F,
custom.labels=c(""))
par(new=F)
}
# estimate effects
modelEffects <- estimateEffect(formula=1:k~positive*negative,
stmobj = myModel,
metadata = sentiment_df)
# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
plot.estimateEffect(modelEffects,
covariate ="positive*negative",
xlim=c(-.25,.25),
model = myModel,
topics = modelEffects$topics[i],
method = "difference",
cov.value1 = 1,
cov.value2=0,
main = myTopicLabels[i],
printlegend=F,
linecol="grey26",
labeltype="custom",
verbose.labels=F,
custom.labels=c(""))
par(new=F)
}
# estimate effects
modelEffects <- estimateEffect(formula=1:k~negative,
stmobj = myModel,
metadata = sentiment_df)
# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
plot.estimateEffect(modelEffects,
covariate ="negative",
xlim=c(-.25,.25),
model = myModel,
topics = modelEffects$topics[i],
method = "difference",
cov.value1 = 1,
cov.value2=0,
main = myTopicLabels[i],
printlegend=F,
linecol="grey26",
labeltype="custom",
verbose.labels=F,
custom.labels=c(""))
par(new=F)
}
# estimate effects
modelEffects <- estimateEffect(formula=1:k~positive,
stmobj = myModel,
metadata = sentiment_df)
# plot effects
myRows <- 2
par(mfrow=c(myRows,3), bty="n", lwd=2)
for (i in 1:k){
plot.estimateEffect(modelEffects,
covariate ="positive",
xlim=c(-.25,.25),
model = myModel,
topics = modelEffects$topics[i],
method = "difference",
cov.value1 = 1,
cov.value2=0,
main = myTopicLabels[i],
printlegend=F,
linecol="grey26",
labeltype="custom",
verbose.labels=F,
custom.labels=c(""))
par(new=F)
}
library(tidyverse)
library(quanteda)
library(tm)
library(tidytext)
library(cluster)
library(factoextra)
dfm_matrix <- as.matrix(lyrics_dfm)
scaled_matrix <- scale(dfm_matrix)
scaled_matrix[is.na(scaled_matrix)] <- colMeans(scaled_matrix, na.rm = TRUE)[col(scaled_matrix)[is.na(scaled_matrix)]]
# Replace NaN with 0
scaled_matrix[is.nan(scaled_matrix)] <- 0
# Replace Inf with 0
scaled_matrix[is.infinite(scaled_matrix)] <- 0
# estimate k-means
set.seed(54321)
k_clusters <- 5
kmeans_result <- kmeans(scaled_matrix, centers = k_clusters, nstart = 25)
# visualize clustering
fviz_cluster(list(data = scaled_matrix, cluster = kmeans_result$cluster))
non_constant_cols <- apply(scaled_matrix, 2, var) != 0
scaled_matrix <- scaled_matrix[, non_constant_cols]
# estimate k-means
set.seed(54321)
k_clusters <- 5
kmeans_result <- kmeans(scaled_matrix, centers = k_clusters, nstart = 25)
# visualize clustering
fviz_cluster(list(data = scaled_matrix, cluster = kmeans_result$cluster))
non_constant_cols <- apply(scaled_matrix, 2, var) != 0
scaled_matrix <- scaled_matrix[, non_constant_cols]
# estimate k-means
set.seed(54321)
k_clusters <- 10
kmeans_result <- kmeans(scaled_matrix, centers = k_clusters, nstart = 25)
# visualize clustering
fviz_cluster(list(data = scaled_matrix, cluster = kmeans_result$cluster))
non_constant_cols <- apply(scaled_matrix, 2, var) != 0
scaled_matrix <- scaled_matrix[, non_constant_cols]
# estimate k-means
set.seed(54321)
k_clusters <- 10
kmeans_result <- kmeans(scaled_matrix, centers = k_clusters, nstart = 20)
# visualize clustering
fviz_cluster(list(data = scaled_matrix, cluster = kmeans_result$cluster))
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 1)
lda_model$get_top_words(n = 10, topic_number = c(1L, 3L, 10L), lambda = 0.4)
custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x", "hey")
all_stopwords <- c(stopwords("english"), custom_stopwords)
# tokenize
lyric_tokens <- tokens(sentiment_df$Text,
remove_numbers = T,
remove_punct =  T,
remove_symbols = T)
# trim stop words
lyric_tokens <- tokens_remove(lyric_tokens,
all_stopwords)
# create dfm
myDfm <- dfm(lyric_tokens,
tolower=T)
dim(myDfm)
peace_corp <- corpus(kwic_peace)
peace_tokens <- tokens(peace_corp)
peace_dfm <- dfm(peace_tokens)
textplot_wordcloud(peace_dfm, min_count = 3, random_order = FALSE)
peace_corp <- corpus(kwic_peace)
peace_tokens <- tokens(peace_corp)
peace_dfm <- dfm(peace_tokens)
textplot_wordcloud(peace_dfm, min_count = 2, random_order = FALSE)
peace_corp <- corpus(kwic_peace)
peace_tokens <- tokens(peace_corp)
peace_dfm <- dfm(peace_tokens)
textplot_wordcloud(peace_dfm, min_count = 3, random_order = FALSE)
war_corp <- corpus(kwic_war)
war_tokens <- tokens(war_corp)
war_dfm <- dfm(war_tokens)
textplot_wordcloud(war_dfm, min_count = 3, random_order = FALSE)
war_corp <- corpus(kwic_war)
war_tokens <- tokens(war_corp)
war_dfm <- dfm(war_tokens)
textplot_wordcloud(war_dfm, min_count = 5, random_order = FALSE)
install.packages(c("tesseract", "pdftools"))
library(tesseract)
library(pdftools)
pdf_file <- c("/Users/sophieryan/Documents/Github/Blogs/Extremist_Activities_Are_Threa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Nixon_Says_It's_Time_to_Draw_L.pdf", "/Users/sophieryan/Documents/Github/Blogs/Most_of_U.S._is_behind_anti-wa.pdf")  # Replace with your PDF file path
file.exists(pdf_file)
text <- ocr(pdf_file)
# Display the extracted text
cat(text)
writeLines(text, "ocr_output.txt")
newspaper_corpus <- corpus(text)
summary(newspaper_corpus)
ndoc(newspaper_corpus)
sum(ntoken(newspaper_corpus))
news_tokens_clean <- tokens(newspaper_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
news_tokens_clean <- tokens_select(news_tokens_clean,
pattern = stopwords("en"),
selection = "remove")
news_dfm <- dfm(news_tokens_clean)
topfeatures(news_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 3, random_order = FALSE)
sentiment_scores_news <- dfm_lookup(news_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df_news <- convert(sentiment_scores_news, to = "data.frame")
sentiment_df_news$Text <- as.character(newspaper_corpus)
head(sentiment_df_news)
pdf_file <- c("/Users/sophieryan/Documents/Github/Blogs/Extremist_Activities_Are_Threa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Nixon_Says_It's_Time_to_Draw_L.pdf", "/Users/sophieryan/Documents/Github/Blogs/Most_of_U.S._is_behind_anti-wa.pdf", "/Users/sophieryan/Documents/Github/Blogs/Bravery_medals_discarded_in_an.pdf", "/Users/sophieryan/Documents/Github/Blogs/War_Protesters_Mass_For_Washin-2.pdf",
"/Users/sophieryan/Documents/Github/Blogs/250,000_WAR_PROTESTERS_STAGE_P.pdf", "/Users/sophieryan/Documents/Github/Blogs/WAR_RALLY_TODAY_EXPECTS_NEW_AI.pdf","/Users/sophieryan/Documents/Github/Blogs/Agnew_Scores_War_Foes;_Rally_t.pdf")
file.exists(pdf_file)
text <- ocr(pdf_file)
# Display the extracted text
cat(text)
writeLines(text, "ocr_output.txt")
newspaper_corpus <- corpus(text)
summary(newspaper_corpus)
ndoc(newspaper_corpus)
sum(ntoken(newspaper_corpus))
news_tokens_clean <- tokens(newspaper_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
news_tokens_clean <- tokens_select(news_tokens_clean,
pattern = stopwords("en"),
selection = "remove")
news_dfm <- dfm(news_tokens_clean)
topfeatures(news_dfm)
news_tokens_clean <- tokens(newspaper_corpus,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE) %>%
tokens_tolower()
news_custom_stopwords <- c(stopwords("en"), "ee", "ae", "said", "se", "re", "oe", "eee")
news_tokens_clean <- tokens_select(news_tokens_clean,
pattern = news_custom_stopwords,
selection = "remove")
news_dfm <- dfm(news_tokens_clean)
topfeatures(news_dfm)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 3, random_order = FALSE)
library(quanteda.textplots)
set.seed(1234)
# draw the wordcloud
textplot_wordcloud(news_dfm, min_count = 10, random_order = FALSE)
sentiment_scores_news <- dfm_lookup(news_dfm, dictionary = data_dictionary_LSD2015)
# convert to df
sentiment_df_news <- convert(sentiment_scores_news, to = "data.frame")
sentiment_df_news$Text <- as.character(newspaper_corpus)
head(sentiment_df_news)
sentiment_df_news$polarity <- (sentiment_df_news$positive - sentiment_df_news$negative)/(sentiment_df_news$positive + sentiment_df_news$negative)
ggplot(sentiment_df_news) +
geom_histogram(aes(polarity)) +
theme_bw()
textplot_wordcloud(news_dfm, min_count = 20, random_order = FALSE)
news_fcm <- fcm(news_dfm)
myFeatures <- names(topfeatures(news_dfm, 30))
# retain only those top features as part of our matrix
smaller_fcm_news <- fcm_select(news_fcm, pattern = myFeatures, selection = "keep")
# compute size weight for vertices in network
size <- log(colSums(smaller_fcm_news))
# create plot
textplot_network(smaller_fcm_news, vertex_size = size / max(size) * 3)
kwic_news_peace <- kwic(news_tokens_clean,
pattern = c("peace"))
# look at the first few uses
head(kwic_news_peace)
# now look at a broader window of terms
kwic_news_peace <- kwic(news_tokens_clean,
pattern = c("peace"),
window = 10)
# look at the first few uses
head(kwic_news_peace)
news_peace_corp <- corpus(kwic_news_peace)
news_peace_tokens <- tokens(news_peace_corp)
news_peace_dfm <- dfm(news_peace_tokens)
textplot_wordcloud(news_peace_dfm, min_count = 3, random_order = FALSE)
