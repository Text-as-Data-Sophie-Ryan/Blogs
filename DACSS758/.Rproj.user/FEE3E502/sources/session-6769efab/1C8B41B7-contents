---
title: "Counterculture: Music vs. Media"
format: html
editor: visual
---

# Counterculture: Music vs. Media

## Research Question

In my project, I will be investigating how themes and sentiments of counterculture rock music in the 1960's compared to that of mainstream media, particularly in relation to the Vietnam War, civil rights, and social justice movements.

## My Corpus

In order to collect this data, I decided to utilize the set list from the Woodstock Art and Music Festival of 1969. This festival was a pivotal moment in music history and popular culture. It was representative of the wide social unrest taking place at the time. Almost 500,000 people attended in efforts to promote peace and unity. This was the pinnacle of the counterculture movement, a political and cultural phenomenon that was anti-war, anti-establishment, and pro-peace. Young people across the country organized protests to fight for civil rights, and music often played a huge role in this. Many rock artists at this time used their platforms to support these causes, and the music was very politically charged. Jimi Hendrix' famous rendition of the Star Spangled Banner is one example of how the festival was leveraged to express unhappiness with the political climate. Therefore, the music played at this 3 day long festival is the perfect culmination of songs for my corpus representing the messages of counterculture's rock music.

### Lyrics Corpus

To acquire these songs and lyrics, I used webscraping from Wikipedia and a lyric website called LyricsFreak. First, I scraped the Wikipedia page that gave a list of songs and artists.

```{r}
#Loading packages
library(rvest)
library(dplyr)
library(stringr)
library(purrr)
library(dplyr)
library(quanteda.textplots)
library(quanteda.sentiment)

#Defining the Wikipedia URL for the setlist at Woodstock
url1 <- "https://en.wikipedia.org/wiki/List_of_performances_and_events_at_Woodstock_Festival"

#Defining the CSS selectors. 
css_selector <- ".mw-body p + ol"
css_selector_artists <- ".mw-body .mw-heading3"
special_css <- "ol"
```

I defined 3 selectors, one for the titles which gave the artist name, another for the body which gave the song list, and a third for a special case, Crosby Stills and Nash. This artist's songs were in a nested list, which I had to treat differently.

```{r}
#Song text
song_list <- url1 %>%
  read_html() %>%
  html_nodes(css = css_selector) %>%
  html_text()
  
#CSN song text
special_case <- url1 %>%
  read_html() %>%
  html_nodes(css = special_css) %>%
  html_text() 

#Artist list
artist_list <- url1 %>%
  read_html() %>%
  html_nodes(css = css_selector_artists) %>%
  html_text()

#Removing a speaker, not an artist
artist_list <- artist_list[-2]

#Defining the nested list I need
special2 <- (special_case[29:32])
flattened_songs <- paste(special2, collapse = "\n")

#Adding the nested list songs to the original song list in the corresponding artist spot
song_list <- append(song_list, list(flattened_songs), after = 28)

# Remove [edit] from the artist names
artist_list <- gsub("\\[edit\\]", "", artist_list)
artist_list <- str_squish(artist_list)

split_songs <- str_split(song_list, "\n")

# Clean up whitespace and remove empty rows
clean_songs <- map(split_songs, ~ str_squish(.x) %>% .[. != ""])

# Only keep artists with corresponding song lists
# Check which artists actually have songs listed
valid_artists <- artist_list[1:length(clean_songs)]

# Combine only valid pairs
woodstock_data <- map2(
  valid_artists,
  clean_songs,
  ~ data.frame(artist = .x, song = .y, stringsAsFactors = FALSE)
) %>%
  bind_rows()

# Display the first 10 rows
head(woodstock_data, 10)
```

Now I have a data frame consisting of every artist and each song they played. Next, I had to add lyrics to the data frame. I did this by creating a function that scraped the lyrics from pages on LyricsFreak for every song.

```{r}
#Loading packages
library(httr)
library(curl)

#Creating a function to scrape the LyricsFreak pages for each song. 
get_lyrics_lyricsfreak <- function(song_name, artist_name) {
  # Construct search URL
  base_search_url <- "https://www.lyricsfreak.com/search.php"
  query <- paste(artist_name, song_name)
  
  # Send search request
  search_page <- GET(base_search_url, query = list(q = query))
  
  # Parse the search result page
  search_html <- read_html(content(search_page, as = "text"))
  
  # Get the first song link that matches the artist
  song_link <- search_html %>%
    html_nodes("a.song") %>%
    html_attr("href") %>%
    .[1]  # Get the first match
  
  # If no link found, return NA
  if (is.na(song_link) || length(song_link) == 0) {
    return(NA)
  }
  
  # Complete the URL for the song's page
  song_url <- paste0("https://www.lyricsfreak.com", song_link)
  
  # Scrape the lyrics from the song's page
  song_page <- read_html(song_url)
  
  lyrics <- song_page %>%
    html_nodes(".js-share-text") %>%
    html_text2()
  
  # Clean up the lyrics text
  lyrics <- str_squish(lyrics)
  
  return(lyrics)
}


# Initialize a column to store lyrics
woodstock_data$lyrics <- NA

# Loop through each row and get lyrics
for (i in 1:nrow(woodstock_data)) {
  song_name <- woodstock_data$song[i]
  artist_name <- woodstock_data$artist[i]
  
  # Get the lyrics and store it in the new column
  woodstock_data$lyrics[i] <- get_lyrics_lyricsfreak(song_name, artist_name)
  
  # Pause to be polite to the website
  Sys.sleep(2)
}
```

```{r}
head(woodstock_data)
```

Running this function added an empty column to my existing data frame, and added the lyrics for each song. I am running into some problems at this step, as 161 of the songs do not have lyrics. Going forward, I may have to scrape from other websites to fill in gaps. For now, I have enough lyrics to create a lyrics corpus and perform some pre-processing steps.

```{r}
#First I removed NA and missing values 
cleaned_data <- woodstock_data %>%
  filter(!is.na(lyrics) & lyrics != "")

cleaned_data$doc_id <- paste(cleaned_data$artist, cleaned_data$song, sep = " - ")

#Creating the corpus
lyrics_corpus <- corpus(cleaned_data$lyrics, 
                        docnames = cleaned_data$doc_id)

summary(lyrics_corpus)
ndoc(lyrics_corpus)
sum(ntoken(lyrics_corpus))
```

There are 146 documents and 31958 tokens in my corpus.

Next, I removed numbers, punctuation, and stopwords. I added some custom stopwords, as song lyrics tend to have some repetitive terms that are not very representative of the messages of the songs.

```{r}
tokens_clean <- tokens(lyrics_corpus, 
                       what = "word", 
                       remove_numbers = TRUE,
                       remove_punct = TRUE) %>%
                tokens_tolower()

custom_stopwords <- c(stopwords("en"), "baby", "na", "lyrics", "oh", "got", "x", "yeah", "can", "go", "like", "now", "chorus", "get", "one", "x")

tokens_clean <- tokens_select(tokens_clean,
                              pattern = custom_stopwords,
                              selection = "remove")
```

Then, I created a document feature matrix and found some top features.

```{r}
lyrics_dfm <- dfm(tokens_clean)
topfeatures(lyrics_dfm)
```

Next, I created a few preliminary visualizations to get an idea of what I'll be working with. I made a wordcloud and a network. Moving forward, I'll have to figure out ways I can remove more custom "stopwords" that appear in almost all songs but do not add value.

```{r}
library(quanteda.textplots)

set.seed(1234)

# draw the wordcloud
textplot_wordcloud(lyrics_dfm, min_count = 20, random_order = FALSE)
```

```{r}
smaller_dfm <- dfm_trim(lyrics_dfm, min_termfreq = 10)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .2, docfreq_type = "prop")

# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)

myFeatures <- names(topfeatures(smaller_dfm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_smaller_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)
```

Finally I found some sentiment scores.

```{r}
sentiment_scores <- dfm_lookup(lyrics_dfm, dictionary = data_dictionary_LSD2015)

# convert to df
sentiment_df <- convert(sentiment_scores, to = "data.frame")

sentiment_df$Text <- as.character(lyrics_corpus)

head(sentiment_df)
```

There is still a lot of work that needs to be done on this corpus, but I believe I'm making good progress and this will be a good representation of the themes of counterculture music.

### Media Corpus

For my corpus of mainstream media about counterculture, I would like to use historical newspapers from ProQuest. I was exploring many of these documents using search terms such as "Vietnam War", "antiwar", "protest", "hippie", "counterculture", etc. There were hundreds of relevant documents. Unfortunately, most of these are in pdf form. This makes creating the corpus more difficult. Going forward, I will either have to perform OCR, or find another source to represent the sentiment of mainstream media during the 60's.

Here's an example of one newspaper article from April 24, 1971.

![](images/Screenshot%202025-02-26%20at%205.01.13%20PM.png)

Documents like this could be really useful in my analysis, and I look forward to continuing work on creating a corpus.

## Conclusion

Using text as data to investigate counterculture will provide a really interesting snapshot of sentiments and themes of the time period. I'm fascinated by this movement, and I believe it was a very important time in America for politics, culture, and rock music. Breaking down these corpora into their core elements can provide a new perspective on counterculture that I haven't previously considered, and I'm excited to see where this project can take me.





